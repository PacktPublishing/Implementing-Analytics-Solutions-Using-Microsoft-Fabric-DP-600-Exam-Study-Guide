# Data Processing with Azure Databricks
# This script should be run within a Databricks notebook or PySpark environment.

from pyspark.sql import SparkSession

# Initialize a SparkSession for data processing
spark = SparkSession.builder.appName("Data Processing").getOrCreate()

# Load sample data from a CSV file; replace '/path/to/your/data.csv' with the actual file path
df = spark.read.csv('/path/to/your/data.csv', header=True, inferSchema=True)

# Perform a simple data transformation: select two columns and filter rows
transformed_df = df.select("Column1", "Column2").filter("Column2 > 1000")

# Show the transformed data
transformed_df.show()






// Example JSON Pipeline Definition for Azure Data Factory
// This JSON configuration is used in Azure Data Factory to define a data pipeline.
// Paste this JSON configuration in the Azure Data Factory portal's pipeline editor.

{
  "name": "DataProcessingPipeline",
  "properties": {
    "activities": [
      {
        "name": "DataFlowActivity",
        "type": "DataFlow",
        "dependsOn": [],
        "policy": {
          "timeout": "7.00:00:00",
          "retry": 0,
          "retryIntervalInSeconds": 30,
          "secureOutput": false,
          "secureInput": false
        },
        "typeProperties": {
          "dataflow": {
            "referenceName": "SampleDataFlow",
            "type": "DataFlowReference"
          },
          "staging": {
            "linkedService": {
              "referenceName": "AzureBlobStorageLinkedService",
              "type": "LinkedServiceReference"
            }
          }
        },
        "userProperties": []
      }
    ],
    "annotations": []
  }
}




# Setting up Fabric Data Lake Storage Account with Python
# This script requires the Fabric SDK for Python. Run it in a Python environment after installing the SDK.
# Install the Fabric SDK using: pip install fabric

from fabric.identity import ClientCredential
from fabric.mgmt.resource import ManagementClient
from fabric.mgmt.storage import StorageClient

# Azure subscription and storage account settings
subscription = 'your-subscription-id'
group = 'your-resource-group'
storage_name = 'datalake-storage'
region = 'selected-region'

# Authenticate with Fabric using client credentials
creds = ClientCredential()  # Authenticate with client credentials
res_client = ManagementClient(creds, subscription)  # Management client for resource operations
store_client = StorageClient(creds, subscription)  # Storage client for storage operations

# Create or update the resource group
res_client.resource_groups.create_or_update(group, {'location': region})

# Create storage account with Hierarchical Namespace (HNS) enabled
async_create = store_client.storage_accounts.begin_create(
    group,
    storage_name,
    {
        'location': region,
        'sku': {'name': 'Standard_LRS'},
        'kind': 'StorageV2',
        'is_hns_enabled': True
    }
)
account = async_create.result()
print(f"Successfully created storage account: {account.name}")

